{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGe3GkbCEUZ4tEx+fRiwdC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaM-C/computational-intelligence/blob/main/CIFARPSO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import operator\n",
        "import random\n",
        "import numpy\n",
        "import math\n",
        "!pip install deap\n",
        "from deap import base\n",
        "from deap import benchmarks\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAutoaKj-Y3n",
        "outputId": "f6c2bfb0-4d04-4046-8807-f451d57ffacd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to the range [-1, 1]\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load CIFAR-10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define classes in CIFAR-10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0siyubDDzAx",
        "outputId": "c878780f-41bc-458d-f778-5d6f8eaef9ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, freeze_last_layer=False):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        if freeze_last_layer:\n",
        "            # Freeze all layers except the last layer (fc2)\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.fc2.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rNPLkzPVBT7A"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classifier(freeze_last_layer=False)"
      ],
      "metadata": {
        "id": "A6y2LVVFFKEZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "aexX5df-5kXU",
        "outputId": "e5811a59-57db-459a-a842-f6703df72525"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-0b05a5c67816>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mevery_individual\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevery_individual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevery_generation\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-0b05a5c67816>\u001b[0m in \u001b[0;36mupdateParticle\u001b[0;34m(part, best, weight, freeze_last_layer)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Update only non-frozen parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfreeze_last_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Classifier:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([32, 3, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256, 4096]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([10, 256]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([10])."
          ]
        }
      ],
      "source": [
        "# initialise 50 particles with dimension 48\n",
        "posMinInit      = -3\n",
        "posMaxInit      = + 5\n",
        "VMaxInit        = 1.5\n",
        "VMinInit        = 0.5\n",
        "populationSize  = 50\n",
        "dimension       = sum(param.numel() for param in Classifier().parameters())\n",
        "interval        = 10\n",
        "iterations      = 400\n",
        "\n",
        "#Parameter setup\n",
        "wmax = 0.9 #weighting\n",
        "wmin = 0.4\n",
        "c1   = 2.0\n",
        "c2   = 2.0\n",
        "\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) # -1 is for minimise\n",
        "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
        "\n",
        "# start generating and modifying their positions\n",
        "def generate(size, smin, smax):\n",
        "  part = creator.Particle(random.uniform(posMinInit, posMaxInit) for _ in range(size))\n",
        "  part.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(size)]\n",
        "  part.smin = smin #speed clamping values\n",
        "  part.smax = smax\n",
        "  return part\n",
        "\n",
        "def evaluate(individual, freeze_last_layer=False):\n",
        "    params = [torch.tensor(p, dtype=torch.float32) for p in individual]\n",
        "\n",
        "    # Load the state_dict excluding frozen parameters\n",
        "    state_dict = model.state_dict()\n",
        "    if freeze_last_layer:\n",
        "        state_dict.update({'fc2.weight': torch.tensor(params[-2].view(model.fc2.weight.shape)),\n",
        "                           'fc2.bias': torch.tensor(params[-1])})\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use the DataLoader to iterate through the batches in the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    average_loss = running_loss / len(testloader)\n",
        "\n",
        "    # return accuracy, average_loss\n",
        "    return (1.0/loss.item()),\n",
        "\n",
        "def updateParticle(part, best, weight, freeze_last_layer=False):\n",
        "    #implementing speed = 0.7*(weight*speed + c1*r1*(localBestPos-currentPos) + c2*r2*(globalBestPos-currentPos))\n",
        "    #Note that part and part.speed are both lists of size dimension\n",
        "    #hence all multiplies need to apply across lists, so using e.g. map(operator.mul, ...\n",
        "\n",
        "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
        "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
        "\n",
        "    v_r0 = [weight*x for x in part.speed]\n",
        "    v_r1 = [c1*x for x in map(operator.mul, r1, map(operator.sub, part.best, part))] # local best\n",
        "    v_r2 = [c2*x for x in map(operator.mul, r2, map(operator.sub, best, part))] # global best\n",
        "\n",
        "    part.speed = [0.7*x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
        "    # update position with speed\n",
        "    part[:] = list(map(operator.add, part, part.speed))\n",
        "\n",
        "    # Transform the particle position to the neural network parameters\n",
        "    params = [torch.tensor(p, dtype=torch.float32) for p in part]\n",
        "\n",
        "    # Update only non-frozen parameters\n",
        "    if not freeze_last_layer:\n",
        "        model.load_state_dict({key: value for key, value in zip(model.state_dict(), params)})\n",
        "    else:\n",
        "        model.load_state_dict({key: value for key, value in zip(model.state_dict(), params[:-2])})\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"particle\", generate, size=dimension, smin=-3, smax=3)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
        "toolbox.register(\"update\", updateParticle)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "\n",
        "pop = toolbox.population(n=populationSize) # Population Size\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", numpy.mean)\n",
        "stats.register(\"std\", numpy.std)\n",
        "stats.register(\"min\", numpy.min)\n",
        "stats.register(\"max\", numpy.max)\n",
        "\n",
        "logbook = tools.Logbook()\n",
        "logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
        "\n",
        "best = None\n",
        "\n",
        "running_loss=[]\n",
        "x_generations=10\n",
        "\n",
        "for every_generation in range(x_generations):\n",
        "  w = wmax - (wmax-wmin)*every_generation/x_generations #decaying inertia weight\n",
        "\n",
        "\n",
        "  # calculate an individuals fitness. fitness = loss function = CrossEntropy\n",
        "  # start off with random weights in the network\n",
        "  for every_individual in pop:\n",
        "    # the weights start random. But each individuals weights get added to the last\n",
        "    # layer of the network\n",
        "    # predicted = NN(input)\n",
        "    # loss=CrossEntropy(predicted, label)\n",
        "    # running_loss += loss\n",
        "\n",
        "    every_individual.fitness.values = toolbox.evaluate(every_individual)\n",
        "\n",
        "    if (not every_individual.best) or (every_individual.best.fitness < every_individual.fitness):\n",
        "                every_individual.best = creator.Particle(every_individual)\n",
        "                every_individual.best.fitness.values = every_individual.fitness.values\n",
        "\n",
        "    if (not best) or best.fitness < every_individual.fitness:\n",
        "                best = creator.Particle(every_individual)\n",
        "                best.fitness.values = every_individual.fitness.values\n",
        "\n",
        "    for every_individual in pop:\n",
        "            toolbox.update(every_individual, best, w)\n",
        "\n",
        "    if every_generation % interval == 0:\n",
        "            logbook.record(gen=every_generation, evals=len(pop), **stats.compile(pop))\n",
        "            print(logbook.stream)\n",
        "\n",
        "    print('best particle position is ', best)\n",
        "\n",
        "    print(\"Populaiton: \", pop)\n",
        "    print(\"Logbook: \",logbook)\n",
        "    print(\"Best: \", best)\n",
        "\n",
        "\n",
        "    # TO DO:\n",
        "    # get individuals weights\n",
        "    # put them into the network\n"
      ]
    }
  ]
}