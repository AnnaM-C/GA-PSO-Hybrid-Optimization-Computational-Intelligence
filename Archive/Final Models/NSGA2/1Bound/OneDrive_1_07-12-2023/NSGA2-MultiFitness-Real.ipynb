{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    This file is part of DEAP.\n",
    "#    This implements the NSGA-II in an easy way because it makes us of DEAP subroutines\n",
    "#    The non dominated sort and crowding distance are solved by a simiple call to DEAP subroutines\n",
    "#    and their implementation is hidden.\n",
    "#\n",
    "#    DEAP is free software: you can redistribute it and/or modify\n",
    "#    it under the terms of the GNU Lesser General Public License as\n",
    "#    published by the Free Software Foundation, either version 3 of\n",
    "#    the License, or (at your option) any later version.\n",
    "#\n",
    "#    DEAP is distributed in the hope that it will be useful,\n",
    "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "#    GNU Lesser General Public License for more details.\n",
    "#\n",
    "#    You should have received a copy of the GNU Lesser General Public\n",
    "#    License along with DEAP. If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "import array\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import numpy\n",
    "\n",
    "from math import sqrt\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of loaders to create: 50\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torchvision.__version__\n",
    "\n",
    "#normalise pixel values (range:-1 to 1, mean:0.5, s.d:0.5)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size=10000\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "totalImages   = len(trainloader.dataset)\n",
    "miniAmount    = 1000\n",
    "numOfLoaders  = totalImages // miniAmount\n",
    "\n",
    "print(\"Number of loaders to create: \" + str(numOfLoaders)) # 10 Loaders\n",
    "\n",
    "miniLoaders = []\n",
    "\n",
    "for i in range(numOfLoaders):\n",
    "  startIdx        = i * miniAmount\n",
    "  endIdx          = (i + 1) * miniAmount if i < numOfLoaders - 1 else totalImages\n",
    "  subset          = torch.utils.data.Subset(trainset, range(startIdx, endIdx))  # Creates a subset of the whole training set\n",
    "  SubTrainLoader  = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "  miniLoaders.append(SubTrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=52, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=52, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 52),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(52, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "PopModel=Net()\n",
    "PopModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1994426/1995406619.py:7: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(final_layer.weight)\n"
     ]
    }
   ],
   "source": [
    "PATH = ('./40_epoch_32_batch_SGD_net.pth')\n",
    "PopModel = Net()\n",
    "PopModel.load_state_dict(torch.load(PATH))\n",
    "final_layer = PopModel.classifier[-1]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "nn.init.xavier_uniform(final_layer.weight)\n",
    "\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
    "# creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMulti)\n",
    "NDIM = 530\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "BOUND_LOW, BOUND_UP = -1.0, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(low, up, size=None):\n",
    "    try:\n",
    "        return [random.uniform(a, b) for a, b in zip(low, up)]\n",
    "    except TypeError:\n",
    "        return [random.uniform(a, b) for a, b in zip([low] * size, [up] * size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel():\n",
    "    runningValLoss=0.0\n",
    "    total=0.0\n",
    "    PopModel.eval()\n",
    "    correct_predictions=0.0\n",
    "    total_samples=0.0\n",
    "    correct_predictions=0.0\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            PopModel.to(device)\n",
    "            prediction = PopModel(images)\n",
    "            loss=criterion(prediction, labels)\n",
    "            runningValLoss=loss.item()\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100*(correct_predictions / total_samples)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFitness(individual, miniNumber):\n",
    "    weights = numpy.square(numpy.array(individual))\n",
    "    particleweightsNP1 = numpy.array(individual)\n",
    "    particleweightsNP = particleweightsNP1[:520]\n",
    "    biases = numpy.array(particleweightsNP1[-10:])\n",
    "    biases = torch.from_numpy(biases).float()\n",
    "    final_layer.bias = torch.nn.Parameter(biases.float())\n",
    "\n",
    "    #converting to the correct shape\n",
    "    reshapedWeights = particleweightsNP.reshape(10,52)\n",
    "\n",
    "    #converting to torch array\n",
    "    torchWeights = torch.from_numpy(reshapedWeights).float()\n",
    "\n",
    "    #setting the weights of the final layer to these weights\n",
    "    final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "#     print(\"Calc for Individual\")\n",
    "    with torch.no_grad():\n",
    "        for data in miniLoaders[miniNumber]:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            PopModel.to(device)\n",
    "            outputs = PopModel(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100*(correct_predictions / total_samples)\n",
    "#     print(f\"For entire training set, total samples: {total_samples}, correct predictions: {correct_predictions}, accuracy: {accuracy}\")\n",
    "    regularizer = numpy.sum(weights)\n",
    "\n",
    "    return (accuracy, regularizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox.register(\"attr_float\", uniform, BOUND_LOW, BOUND_UP, NDIM)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.attr_float)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", calcFitness)\n",
    "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0)\n",
    "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0, indpb=1.0/NDIM)\n",
    "toolbox.register(\"select\", tools.selNSGA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed=None):\n",
    "    random.seed(seed)\n",
    "\n",
    "    NGEN = 100\n",
    "    MU =  100\n",
    "    CXPB = 0.8\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", numpy.min, axis=0)\n",
    "    stats.register(\"max\", numpy.max, axis=0)\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = \"gen\", \"evals\", \"min\", \"max\"\n",
    "    \n",
    "    pop = toolbox.population(n=MU)\n",
    "    miniCounter = 0\n",
    "    \n",
    "    #evaluating the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "    print(\"Evaluate fitnesses before generation loop..\")\n",
    "    # fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
    "\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    #assigning crowding distance to the individuals\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "    \n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, evals=len(invalid_ind), **record)\n",
    "    print(logbook.stream)\n",
    "\n",
    "    #beginning the generational process\n",
    "    for gen in range(1, NGEN):\n",
    "        #vary the population\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
    "        \n",
    "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "        #making pairs of all (even,odd) in offspring\n",
    "            if random.random() <= CXPB:\n",
    "                toolbox.mate(ind1, ind2)\n",
    "            \n",
    "            toolbox.mutate(ind1)\n",
    "            toolbox.mutate(ind2)\n",
    "            del ind1.fitness.values, ind2.fitness.values\n",
    "        \n",
    "        #evaluating the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "#         print(f\"Evaluate fitnesses at {NGEN} generation..\")\n",
    "        # fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
    "\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "#             print(f\"Fitness for individual is {fit}\")\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        #selecting the next generation population\n",
    "        pop = toolbox.select(pop + offspring, MU)\n",
    "\n",
    "        #we now have a new evolved population, the logbook can just record the highest accuracy and lowest regulariser for that generation\n",
    "        #generations continue untill 100\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "\n",
    "        miniCounter += 1\n",
    "\n",
    "        if miniCounter >= (len(miniLoaders)):\n",
    "          miniCounter = 0\n",
    "          print(\"Mini Counter reset!\")\n",
    "\n",
    "    print(\"Final population hypervolume is %f\" % hypervolume(pop, [11.0, 11.0]))\n",
    "\n",
    "    return pop, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate fitnesses before generation loop..\n",
      "gen\tevals\tmin                        \tmax                        \n",
      "0  \t100  \t[  3.5        159.47850187]\t[ 22.8        195.42196383]\n",
      "1  \t100  \t[  2.7        159.47850187]\t[ 22.8        189.91002621]\n",
      "2  \t100  \t[  2.7        154.27499938]\t[ 22.8        189.57436799]\n",
      "3  \t100  \t[  3.3        152.75316254]\t[ 22.8        189.57436799]\n",
      "4  \t100  \t[  6.8        152.41081221]\t[ 22.8        189.57436799]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1994426/1024233786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Clone a all individual from a current popoulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpopclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1994426/4292424515.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiCounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#             print(f\"Fitness for individual is {fit}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1994426/4292424515.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(part)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#         print(f\"Evaluate fitnesses at {NGEN} generation..\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiCounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1994426/859456683.py\u001b[0m in \u001b[0;36mcalcFitness\u001b[0;34m(individual, miniNumber)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pop, stats = main()\n",
    "    # Clone a all individual from a current popoulation\n",
    "    popclone = list(map(toolbox.clone, pop))\n",
    "\n",
    "    pop.sort(key=lambda x: x.fitness.values)\n",
    "    \n",
    "    # print some individuals\n",
    "    for n in range(10):\n",
    "        i=pop[random.choice(range(0, len(pop)))]\n",
    "        x1=i[0:4]\n",
    "        x2=i[4:7]\n",
    "        x3=i[7:9]\n",
    "        x1= int(\"\".join(str(i) for i in x1),2)\n",
    "        x2= int(\"\".join(str(i) for i in x2),2)\n",
    "        x3= int(\"\".join(str(i) for i in x3),2)\n",
    "        print(x1,x2,x3)\n",
    "    \n",
    "    # get a first non dominate individuals in all an optimal solutions\n",
    "    first_front = tools.sortNondominated(individuals=popclone,k=len(popclone), first_front_only=True)[0]\n",
    "    best_weights = first_front[0]\n",
    "    # sort each individual by its ftiness values\n",
    "    first_front.sort(key= lambda x: x.fitness.values)\n",
    "    popclone.sort(key = lambda x: x.fitness.values)\n",
    "\n",
    "    # get both fitness values of each individual in an array\n",
    "    # all individual\n",
    "    all_fronts = numpy.array([ind.fitness.values for ind in popclone])\n",
    "    # Non dominated individual\n",
    "    first_front = numpy.array([ ind.fitness.values for ind in first_front])\n",
    "\n",
    "\n",
    "    fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
    "    ax.scatter(all_fronts[:,0], all_fronts[:,1], c='b', marker='x')\n",
    "    ax.scatter(first_front[:,0], first_front[:,1], s=150, facecolor='none', edgecolors='g', linewidths=2)\n",
    "    ax.plot(first_front[:,0], first_front[:,1], c='r', linestyle='dashdot')\n",
    "    ax.set_title(\"NSGA II Pareto front\")\n",
    "    ax.set_xlabel('Accuracy (objective 1)')\n",
    "    ax.set_ylabel(\"Regulariser (objective 2)\")\n",
    "    ax.grid()\n",
    "    #plt.axis(\"tight\")\n",
    "    figure = ax.get_figure()\n",
    "    plt.show()\n",
    "\n",
    "    #set the weights in the model\n",
    "    weights = numpy.square(numpy.array(best_weights))\n",
    "    particleweightsNP1 = numpy.array(best_weights)\n",
    "    particleweightsNP = particleweightsNP1[:520]\n",
    "    biases = numpy.array(particleweightsNP1[-10:])\n",
    "    biases = torch.from_numpy(biases).float()\n",
    "    final_layer.bias = torch.nn.Parameter(biases.float())\n",
    "\n",
    "    # Convert to the correct shape\n",
    "    reshapedWeights = particleweightsNP.reshape(10,52)\n",
    "\n",
    "    # Convert to torch array\n",
    "    torchWeights = torch.from_numpy(reshapedWeights).float()\n",
    "\n",
    "    # Set the weights of the final layer to these weights\n",
    "    final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
    "\n",
    "    #evaluate the model\n",
    "    accuracy = evaluateModel()\n",
    "\n",
    "    print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a first non dominate individuals in all an optimal solutions\n",
    "first_front = tools.sortNondominated(individuals=popclone,k=len(popclone), first_front_only=True)[0]\n",
    "best_weights = first_front[0]\n",
    "# sort each individual by its ftiness values\n",
    "first_front.sort(key= lambda x: x.fitness.values)\n",
    "popclone.sort(key = lambda x: x.fitness.values)\n",
    "\n",
    "# get both fitness values of each individual in an array\n",
    "# all individual\n",
    "all_fronts = numpy.array([ind.fitness.values for ind in popclone])\n",
    "# Non dominated individual\n",
    "first_front = numpy.array([ ind.fitness.values for ind in first_front])\n",
    "\n",
    "\n",
    "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
    "ax.scatter(all_fronts[:,0], all_fronts[:,1], c='b', marker='x')\n",
    "ax.scatter(first_front[:,0], first_front[:,1], s=150, facecolor='none', edgecolors='g', linewidths=2)\n",
    "ax.plot(first_front[:,0], first_front[:,1], c='r', linestyle='dashdot')\n",
    "ax.set_title(\"NSGA II Pareto front\")\n",
    "ax.set_xlabel('Accuracy (objective 1)')\n",
    "ax.set_ylabel(\"Regulariser (objective 2)\")\n",
    "ax.grid()\n",
    "#plt.axis(\"tight\")\n",
    "figure = ax.get_figure()\n",
    "plt.show()\n",
    "\n",
    "#set the weights in the model\n",
    "weights = numpy.square(numpy.array(best_weights))\n",
    "particleweightsNP1 = numpy.array(best_weights)\n",
    "particleweightsNP = particleweightsNP1[:520]\n",
    "biases = numpy.array(particleweightsNP1[-10:])\n",
    "biases = torch.from_numpy(biases).float()\n",
    "final_layer.bias = torch.nn.Parameter(biases.float())\n",
    "\n",
    "# Convert to the correct shape\n",
    "reshapedWeights = particleweightsNP.reshape(10,52)\n",
    "\n",
    "# Convert to torch array\n",
    "torchWeights = torch.from_numpy(reshapedWeights).float()\n",
    "\n",
    "# Set the weights of the final layer to these weights\n",
    "final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
    "\n",
    "#evaluate the model\n",
    "accuracy = evaluateModel()\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = stats.select(\"gen\")\n",
    "evals = stats.select(\"evals\")\n",
    "min_fit = [x[0] for x in stats.select(\"min\")]\n",
    "max_fit = [x[0] for x in stats.select(\"max\")]\n",
    "\n",
    "csv_filename = \"training_log_6.csv\"\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"gen\", \"evals\", \"min\", \"max\"])\n",
    "    csv_writer.writerows(zip(gen, evals, min_fit, max_fit))\n",
    "\n",
    "print(f\"Logbook exported to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
    "ax.scatter(all_fronts[:,0], all_fronts[:,1], c='b', marker='x')\n",
    "ax.scatter(first_front[:,0], first_front[:,1], s=150, facecolor='none', edgecolors='g', linewidths=2)\n",
    "ax.plot(first_front[:,0], first_front[:,1], c='r', linestyle='dashdot')\n",
    "ax.set_title(\"NSGA II Pareto front\")\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel(\"Sum of square weights of a model\")\n",
    "ax.grid()\n",
    "#plt.axis(\"tight\")\n",
    "figure = ax.get_figure()\n",
    "plt.show()\n",
    "figure.savefig(\"nsga-front.pdf\", bbox_inches='tight')\n",
    "figure.savefig(\"nsga-front.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
    "ax.plot(first_front[:,0], first_front[:,1], c='tab:orange', linestyle='solid')\n",
    "ax.scatter(first_front[:,0], first_front[:,1], s=100, facecolor='tab:orange', edgecolors='tab:orange', linewidths=2)\n",
    "ax.scatter(all_fronts[:,0], all_fronts[:,1], marker='x')\n",
    "ax.scatter(first_front[:,0], first_front[:,1], marker='x', facecolor='w')\n",
    "ax.set_title(\"NSGA II Pareto front\")\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel(\"Sum of square weights of a model\")\n",
    "ax.grid()\n",
    "#plt.axis(\"tight\")\n",
    "figure = ax.get_figure()\n",
    "plt.show()\n",
    "figure.savefig(\"nsga-front.pdf\", bbox_inches='tight')\n",
    "figure.savefig(\"nsga-front.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All fronts, \", all_fronts)\n",
    "print(\"First front, \", first_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_all_fronts = \"all_fronts.csv\"\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "#     csv_writer.writerow([\"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"])\n",
    "    csv_writer.writerows(all_fronts)\n",
    "\n",
    "print(f\"Logbook exported to {csv_all_fronts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphEpochCounter = range(100)\n",
    "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
    "ax.fill_between(graphEpochCounter, min_fit, max_fit, alpha=0.2)\n",
    "ax.plot(graphEpochCounter, avg)\n",
    "ax.set_title(\"NSGA-II Training Accuracy\")\n",
    "ax.set_xlabel('Generations')\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.grid()\n",
    "figure = ax.get_figure()\n",
    "plt.show()\n",
    "figure.savefig(\"nsga-accuracy.pdf\", bbox_inches='tight')\n",
    "figure.savefig(\"nsga-accuracy.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
