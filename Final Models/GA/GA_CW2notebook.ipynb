{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R88ypjbboVdk"
      },
      "outputs": [],
      "source": [
        "#    This file is part of DEAP.\n",
        "#    This implements the NSGA-II in an easy way because it makes us of DEAP subroutines\n",
        "#    The non dominated sort and crowding distance are solved by a simiple call to DEAP subroutines\n",
        "#    and their implementation is hidden.\n",
        "#\n",
        "#    DEAP is free software: you can redistribute it and/or modify\n",
        "#    it under the terms of the GNU Lesser General Public License as\n",
        "#    published by the Free Software Foundation, either version 3 of\n",
        "#    the License, or (at your option) any later version.\n",
        "#\n",
        "#    DEAP is distributed in the hope that it will be useful,\n",
        "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
        "#    GNU Lesser General Public License for more details.\n",
        "#\n",
        "#    You should have received a copy of the GNU Lesser General Public\n",
        "#    License along with DEAP. If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "import array\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import numpy\n",
        "\n",
        "from math import sqrt\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import benchmarks\n",
        "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "torchvision.__version__\n",
        "\n",
        "#normalise pixel values (range:-1 to 1, mean:0.5, s.d:0.5)\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size=10000\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "totalImages   = len(trainloader.dataset)\n",
        "miniAmount    = 1000\n",
        "numOfLoaders  = totalImages // miniAmount\n",
        "\n",
        "print(\"Number of loaders to create: \" + str(numOfLoaders)) # 10 Loaders\n",
        "\n",
        "miniLoaders = []\n",
        "\n",
        "for i in range(numOfLoaders):\n",
        "  startIdx        = i * miniAmount\n",
        "  endIdx          = (i + 1) * miniAmount if i < numOfLoaders - 1 else totalImages\n",
        "  subset          = torch.utils.data.Subset(trainset, range(startIdx, endIdx))  # Creates a subset of the whole training set\n",
        "  SubTrainLoader  = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "  miniLoaders.append(SubTrainLoader)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 52),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(52, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "PopModel=Net()\n",
        "PopModel.to(device)\n",
        "\n",
        "PATH = ('./40_epoch_32_batch_SGD_net.pth')\n",
        "PopModel = Net()\n",
        "PopModel.load_state_dict(torch.load(PATH))\n",
        "final_layer = PopModel.classifier[-1]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "nn.init.xavier_uniform(final_layer.weight)\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "# creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
        "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMax)\n",
        "NDIM = 530\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "BOUND_LOW, BOUND_UP = -1.0, 1.0\n",
        "\n",
        "def uniform(low, up, size=None):\n",
        "    try:\n",
        "        return [random.uniform(a, b) for a, b in zip(low, up)]\n",
        "    except TypeError:\n",
        "        return [random.uniform(a, b) for a, b in zip([low] * size, [up] * size)]\n",
        "\n",
        "\n",
        "\n",
        "def calcFitness(individual, miniNumber):\n",
        "    weights = numpy.square(numpy.array(individual))\n",
        "    particleweightsNP1 = numpy.array(individual)\n",
        "    particleweightsNP = particleweightsNP1[:520]\n",
        "    biases = numpy.array(particleweightsNP1[-10:])\n",
        "    biases = torch.from_numpy(biases).float()\n",
        "    final_layer.bias = torch.nn.Parameter(biases.float())\n",
        "\n",
        "    #converting to the correct shape\n",
        "    reshapedWeights = particleweightsNP.reshape(10,52)\n",
        "\n",
        "    #converting to torch array\n",
        "    torchWeights = torch.from_numpy(reshapedWeights).float()\n",
        "\n",
        "    #setting the weights of the final layer to these weights\n",
        "    final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "#     print(\"Calc for Individual\")\n",
        "    with torch.no_grad():\n",
        "        for data in miniLoaders[miniNumber]:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            PopModel.to(device)\n",
        "            outputs = PopModel(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100*(correct_predictions / total_samples)\n",
        "#     print(f\"For entire training set, total samples: {total_samples}, correct predictions: {correct_predictions}, accuracy: {accuracy}\")\n",
        "\n",
        "    return accuracy,\n",
        "\n",
        "toolbox.register(\"attr_float\", uniform, BOUND_LOW, BOUND_UP, NDIM)\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.attr_float)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"evaluate\", calcFitness)\n",
        "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0)\n",
        "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0, indpb=1.0/NDIM)\n",
        "toolbox.register(\"select\", tools.selRoulette, fit_attr='fitness')\n",
        "\n",
        "\n",
        "def main(seed=None):\n",
        "    random.seed(seed)\n",
        "\n",
        "    NGEN = 100\n",
        "    MU =  100\n",
        "    CXPB = 0.8\n",
        "\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"min\", numpy.min, axis=0)\n",
        "    stats.register(\"max\", numpy.max, axis=0)\n",
        "\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = \"gen\", \"evals\", \"min\", \"max\"\n",
        "\n",
        "    pop = toolbox.population(n=MU)\n",
        "    miniCounter = 0\n",
        "\n",
        "    #evaluating the individuals with an invalid fitness\n",
        "    invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
        "    print(\"Evaluate fitnesses before generation loop..\")\n",
        "    # fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "    fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
        "\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    #assigning crowding distance to the individuals\n",
        "    pop = toolbox.select(pop, len(pop))\n",
        "\n",
        "    record = stats.compile(pop)\n",
        "    logbook.record(gen=0, evals=len(invalid_ind), **record)\n",
        "    print(logbook.stream)\n",
        "\n",
        "    #beginning the generational process\n",
        "    for gen in range(1, NGEN):\n",
        "        #vary the population\n",
        "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
        "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
        "\n",
        "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
        "        #making pairs of all (even,odd) in offspring\n",
        "            if random.random() <= CXPB:\n",
        "                toolbox.mate(ind1, ind2)\n",
        "\n",
        "            toolbox.mutate(ind1)\n",
        "            toolbox.mutate(ind2)\n",
        "            del ind1.fitness.values, ind2.fitness.values\n",
        "\n",
        "        #evaluating the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "#         print(f\"Evaluate fitnesses at {NGEN} generation..\")\n",
        "        # fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "        fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
        "\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "#             print(f\"Fitness for individual is {fit}\")\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        #selecting the next generation population\n",
        "        pop = toolbox.select(pop + offspring, MU)\n",
        "\n",
        "        #we now have a new evolved population, the logbook can just record the highest accuracy and lowest regulariser for that generation\n",
        "        #generations continue untill 100\n",
        "        record = stats.compile(pop)\n",
        "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
        "        print(logbook.stream)\n",
        "\n",
        "        miniCounter += 1\n",
        "\n",
        "        if miniCounter >= (len(miniLoaders)):\n",
        "          miniCounter = 0\n",
        "          print(\"Mini Counter reset!\")\n",
        "\n",
        "    print(\"Final population hypervolume is %f\" % hypervolume(pop, [11.0, 11.0]))\n",
        "\n",
        "    return pop, logbook\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pop, stats = main()"
      ]
    }
  ]
}