{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uplVzO6QV7NH"
      },
      "outputs": [],
      "source": [
        "#    This file is part of DEAP.\n",
        "#    This implements the NSGA-II in an easy way because it makes us of DEAP subroutines\n",
        "#    The non dominated sort and crowding distance are solved by a simiple call to DEAP subroutines\n",
        "#    and their implementation is hidden.\n",
        "#\n",
        "#    DEAP is free software: you can redistribute it and/or modify\n",
        "#    it under the terms of the GNU Lesser General Public License as\n",
        "#    published by the Free Software Foundation, either version 3 of\n",
        "#    the License, or (at your option) any later version.\n",
        "#\n",
        "#    DEAP is distributed in the hope that it will be useful,\n",
        "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
        "#    GNU Lesser General Public License for more details.\n",
        "#\n",
        "#    You should have received a copy of the GNU Lesser General Public\n",
        "#    License along with DEAP. If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "import array\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import numpy\n",
        "# !pip install deap\n",
        "from math import sqrt\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import benchmarks\n",
        "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndQ7aAAOV7NJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "torchvision.__version__\n",
        "\n",
        "#normalise pixel values (range:-1 to 1, mean:0.5, s.d:0.5)\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size=10000\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "totalImages   = len(trainloader.dataset)\n",
        "miniAmount    = 1000\n",
        "numOfLoaders  = totalImages // miniAmount\n",
        "\n",
        "print(\"Number of loaders to create: \" + str(numOfLoaders)) # 10 Loaders\n",
        "\n",
        "miniLoaders = []\n",
        "\n",
        "for i in range(numOfLoaders):\n",
        "  startIdx        = i * miniAmount\n",
        "  endIdx          = (i + 1) * miniAmount if i < numOfLoaders - 1 else totalImages\n",
        "  subset          = torch.utils.data.Subset(trainset, range(startIdx, endIdx))  # Creates a subset of the whole training set\n",
        "  SubTrainLoader  = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "  miniLoaders.append(SubTrainLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgwa7rXqV7NL"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 52),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(52, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "PopModel=Net()\n",
        "PopModel.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IWm4vkwV7NM"
      },
      "outputs": [],
      "source": [
        "PATH = ('./40_epoch_32_batch_SGD_net.pth')\n",
        "PopModel = Net()\n",
        "PopModel.load_state_dict(torch.load(PATH))\n",
        "final_layer = PopModel.classifier[-1]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "nn.init.xavier_uniform(final_layer.weight)\n",
        "\n",
        "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
        "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMulti)\n",
        "NDIM = 530\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "BOUND_LOW, BOUND_UP = -1.0, 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKdb6DqFV7NN"
      },
      "outputs": [],
      "source": [
        "def uniform(low, up, size=None):\n",
        "    try:\n",
        "        return [random.uniform(a, b) for a, b in zip(low, up)]\n",
        "    except TypeError:\n",
        "        return [random.uniform(a, b) for a, b in zip([low] * size, [up] * size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkBjwIG6V7NN"
      },
      "outputs": [],
      "source": [
        "def evaluateModel():\n",
        "    runningValLoss=0.0\n",
        "    total=0.0\n",
        "    PopModel.eval()\n",
        "    correct_predictions=0.0\n",
        "    total_samples=0.0\n",
        "    correct_predictions=0.0\n",
        "    # forward pass to get predictions\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            PopModel.to(device)\n",
        "            prediction = PopModel(images)\n",
        "            loss=criterion(prediction, labels)\n",
        "            runningValLoss=loss.item()\n",
        "            _, predicted = torch.max(prediction.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100*(correct_predictions / total_samples)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSjBJjcYV7NN"
      },
      "outputs": [],
      "source": [
        "def calcFitness(individual, miniNumber):\n",
        "    weights = numpy.square(numpy.array(individual))\n",
        "    particleweightsNP1 = numpy.array(individual)\n",
        "    particleweightsNP = particleweightsNP1[:520]\n",
        "    biases = numpy.array(particleweightsNP1[-10:])\n",
        "    biases = torch.from_numpy(biases).float()\n",
        "    final_layer.bias = torch.nn.Parameter(biases.float())\n",
        "\n",
        "    #converting to the correct shape\n",
        "    reshapedWeights = particleweightsNP.reshape(10,52)\n",
        "\n",
        "    #converting to torch array\n",
        "    torchWeights = torch.from_numpy(reshapedWeights).float()\n",
        "\n",
        "    #setting the weights of the final layer to these weights\n",
        "    final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in miniLoaders[miniNumber]:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            PopModel.to(device)\n",
        "            outputs = PopModel(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100*(correct_predictions / total_samples)\n",
        "    regularizer = numpy.sum(weights)\n",
        "\n",
        "    return (accuracy, regularizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVbo3UjpV7NN"
      },
      "outputs": [],
      "source": [
        "toolbox.register(\"attr_float\", uniform, BOUND_LOW, BOUND_UP, NDIM)\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.attr_float)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"evaluate\", calcFitness)\n",
        "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0)\n",
        "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0, indpb=1.0/NDIM)\n",
        "toolbox.register(\"select\", tools.selNSGA2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj9dljrZV7NO"
      },
      "outputs": [],
      "source": [
        "def main(seed=None):\n",
        "    random.seed(seed)\n",
        "\n",
        "    NGEN = 100\n",
        "    MU =  100\n",
        "    CXPB = 0.9\n",
        "\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", numpy.mean, axis=0)\n",
        "    stats.register(\"std\", numpy.std, axis=0)\n",
        "    stats.register(\"min\", numpy.min, axis=0)\n",
        "    stats.register(\"max\", numpy.max, axis=0)\n",
        "\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
        "\n",
        "    pop = toolbox.population(n=MU)\n",
        "    miniCounter = 0\n",
        "\n",
        "    # evaluating the individuals with an invalid fitness\n",
        "    invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
        "\n",
        "    print(\"Evaluate fitnesses before generation loop..\")\n",
        "    fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
        "\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # assigning crowding distance to the individuals\n",
        "    pop = toolbox.select(pop, len(pop))\n",
        "\n",
        "    record = stats.compile(pop)\n",
        "    logbook.record(gen=0, evals=len(invalid_ind), **record)\n",
        "    print(logbook.stream)\n",
        "\n",
        "    # begin the generational process\n",
        "    for gen in range(1, NGEN):\n",
        "        # vary population\n",
        "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
        "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
        "\n",
        "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
        "        # make pairs of all (even,odd) in offspring\n",
        "            if random.random() <= CXPB:\n",
        "                toolbox.mate(ind1, ind2)\n",
        "\n",
        "            toolbox.mutate(ind1)\n",
        "            toolbox.mutate(ind2)\n",
        "            del ind1.fitness.values, ind2.fitness.values\n",
        "\n",
        "        # evaluating the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(lambda part: toolbox.evaluate(part, miniCounter), invalid_ind)\n",
        "\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        #selecting the next generation population\n",
        "        pop = toolbox.select(pop + offspring, MU)\n",
        "\n",
        "        # we now have a new evolved population\n",
        "        # record the highest accuracy and lowest regulariser for that generation\n",
        "        # in logbook generations continue untill 100\n",
        "        record = stats.compile(pop)\n",
        "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
        "        print(logbook.stream)\n",
        "\n",
        "        miniCounter += 1\n",
        "\n",
        "        if miniCounter >= (len(miniLoaders)):\n",
        "          miniCounter = 0\n",
        "          print(\"Mini Counter reset!\")\n",
        "\n",
        "    return pop, logbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LJ0YLxoV7NO"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pop, stats = main()\n",
        "    popClone = list(map(toolbox.clone, pop))\n",
        "    pop.sort(key=lambda x: x.fitness.values)\n",
        "\n",
        "    # get a first non dominate individuals in all an optimal solutions\n",
        "    firstFront = tools.sortNondominated(individuals=popClone,k=len(popClone), first_front_only=True)[0]\n",
        "    bestWeights = firstFront[0]\n",
        "\n",
        "    firstFront.sort(key= lambda x: x.fitness.values)\n",
        "    popClone.sort(key = lambda x: x.fitness.values)\n",
        "\n",
        "    # get both acc and reg fitness values of each individual in an array\n",
        "    allFronts = numpy.array([ind.fitness.values for ind in popClone])\n",
        "    firstFront = numpy.array([ ind.fitness.values for ind in firstFront])\n",
        "\n",
        "    # set weights in the model\n",
        "    weights = numpy.square(numpy.array(bestWeights))\n",
        "    particleweightsNP1 = numpy.array(bestWeights)\n",
        "    particleweightsNP = particleweightsNP1[:520]\n",
        "    biases = numpy.array(particleweightsNP1[-10:])\n",
        "    biases = torch.from_numpy(biases).float()\n",
        "    final_layer.bias = torch.nn.Parameter(biases.float())\n",
        "\n",
        "    # convert to the correct shape\n",
        "    reshapedWeights = particleweightsNP.reshape(10,52)\n",
        "    torchWeights = torch.from_numpy(reshapedWeights).float()\n",
        "\n",
        "    # Set the weights of the final layer to these weights\n",
        "    final_layer.weight = torch.nn.Parameter(torchWeights.float())\n",
        "\n",
        "    #evaluate the model\n",
        "    accuracy = evaluateModel()\n",
        "\n",
        "    print(\"Test Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_NKmmWNV7NS"
      },
      "outputs": [],
      "source": [
        "gen = stats.select(\"gen\")\n",
        "evals = stats.select(\"evals\")\n",
        "std = stats.select(\"std\")\n",
        "avg = [x[0] for x in stats.select(\"avg\")]\n",
        "min_fit = [x[0] for x in stats.select(\"min\")]\n",
        "max_fit = [x[0] for x in stats.select(\"max\")]\n",
        "\n",
        "csv_filename = \"training_log_NSGA.csv\"\n",
        "\n",
        "with open(csv_filename, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"gen\", \"evals\", \"avg\", \"min\", \"max\"])\n",
        "    csv_writer.writerows(zip(gen, evals, min_fit, avg, max_fit))\n",
        "\n",
        "print(f\"Logbook exported to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv06D86PV7NT"
      },
      "outputs": [],
      "source": [
        "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
        "ax.plot(firstFront[:,0], firstFront[:,1], c='tab:orange', linestyle='solid')\n",
        "ax.scatter(firstFront[:,0], firstFront[:,1], s=100, facecolor='tab:orange', edgecolors='tab:orange', linewidths=2)\n",
        "ax.scatter(allFronts[:,0], allFronts[:,1], marker='x')\n",
        "ax.scatter(firstFront[:,0], firstFront[:,1], marker='x', facecolor='w')\n",
        "ax.set_title(\"NSGA II Pareto front\")\n",
        "ax.set_xlabel('Accuracy')\n",
        "ax.set_ylabel(\"Sum of square weights of a model\")\n",
        "ax.grid()\n",
        "figure = ax.get_figure()\n",
        "plt.show()\n",
        "figure.savefig(\"nsga-front.pdf\", bbox_inches='tight')\n",
        "figure.savefig(\"nsga-front.png\", dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw5EmIJ8V7NT"
      },
      "outputs": [],
      "source": [
        "print(\"All fronts, \", allFronts)\n",
        "print(\"First front, \", firstFront)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkCXeUVKV7NT"
      },
      "outputs": [],
      "source": [
        "csv_all_fronts = \"all_fronts.csv\"\n",
        "\n",
        "with open(csv_filename, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "#     csv_writer.writerow([\"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"])\n",
        "    csv_writer.writerows(allFronts)\n",
        "\n",
        "print(f\"Logbook exported to {csv_all_fronts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UmzM2OhV7NU"
      },
      "outputs": [],
      "source": [
        "graphEpochCounter = range(100)\n",
        "fig , (ax) = plt.subplots(ncols=1, nrows=1)\n",
        "ax.fill_between(graphEpochCounter, min_fit, max_fit, alpha=0.2)\n",
        "ax.plot(graphEpochCounter, avg)\n",
        "ax.set_title(\"NSGA-II Training Accuracy\")\n",
        "ax.set_xlabel('Generations')\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.grid()\n",
        "figure = ax.get_figure()\n",
        "plt.show()\n",
        "figure.savefig(\"nsga-accuracy.pdf\", bbox_inches='tight')\n",
        "figure.savefig(\"nsga-accuracy.png\", dpi=300, bbox_inches='tight')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
